2023-12-15 18:35:56,727 - Namespace(device='cuda:2', dataset='PEMS08', fewshot=5, prompt=False, years='2016', seed=2016, bs=16, seq_len=12, horizon=12, input_dim=3, output_dim=1, mode='train', max_epochs=200, patience=10, model_name='staeformer', steps_per_day=288, input_embedding_dim=24, tod_embedding_dim=24, dow_embedding_dim=24, spatial_embedding_dim=0, adaptive_embedding_dim=80, feed_forward_dim=256, num_heads=4, num_layers=3, lrate=0.001, dropout=0.1, use_mixed_proj=True, clip_grad_value=5)
2023-12-15 18:35:57,380 - Data shape: (17856, 170, 3)
2023-12-15 18:35:57,581 - Sample num: 892, Batch num: 55
2023-12-15 18:35:57,972 - Sample num: 3567, Batch num: 222
2023-12-15 18:35:59,733 - Sample num: 3566, Batch num: 222
2023-12-15 18:36:19,028 - The arc of model:
STAEformer(
  (input_proj): Linear(in_features=3, out_features=24, bias=True)
  (tod_embedding): Embedding(288, 24)
  (dow_embedding): Embedding(7, 24)
  (output_proj): Linear(in_features=1824, out_features=12, bias=True)
  (attn_layers_t): ModuleList(
    (0-2): 3 x SelfAttentionLayer(
      (attn): AttentionLayer(
        (FC_Q): Linear(in_features=152, out_features=152, bias=True)
        (FC_K): Linear(in_features=152, out_features=152, bias=True)
        (FC_V): Linear(in_features=152, out_features=152, bias=True)
        (out_proj): Linear(in_features=152, out_features=152, bias=True)
      )
      (feed_forward): Sequential(
        (0): Linear(in_features=152, out_features=256, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=256, out_features=152, bias=True)
      )
      (ln1): LayerNorm((152,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((152,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
  (attn_layers_s): ModuleList(
    (0-2): 3 x SelfAttentionLayer(
      (attn): AttentionLayer(
        (FC_Q): Linear(in_features=152, out_features=152, bias=True)
        (FC_K): Linear(in_features=152, out_features=152, bias=True)
        (FC_V): Linear(in_features=152, out_features=152, bias=True)
        (out_proj): Linear(in_features=152, out_features=152, bias=True)
      )
      (feed_forward): Sequential(
        (0): Linear(in_features=152, out_features=256, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=256, out_features=152, bias=True)
      )
      (ln1): LayerNorm((152,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((152,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
)
2023-12-15 18:36:19,029 - The number of parameters: 1223460
2023-12-15 18:36:19,029 - Start training!
